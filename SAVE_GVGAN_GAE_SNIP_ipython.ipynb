{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAVE_GVGAN_GAE_SNIP.ipython",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelestZ/HighResImg_Gen/blob/master/SAVE_GVGAN_GAE_SNIP_ipython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndnWwxWI4MgX",
        "colab_type": "code",
        "outputId": "7f3ba7ea-d6d5-43e0-88d8-24ddc3dd6868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import os    \n",
        "os.chdir(\"/content/gdrive/My Drive/gcn-data/\")\n",
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n",
            " graph_stat.py\t PGG_dblp_version2.zip\t sub.label.dat\n",
            " NDBLP\t\t __pycache__\t\t sub.link.dat\n",
            " PGG_dblp\t'__pycache__ (1)'\t sub.node.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vdJcKtdpshj",
        "colab_type": "code",
        "outputId": "0a581aee-9d1f-46ec-9e4a-bdae9f90cb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip3 install python-igraph\n",
        "!pip3 install powerlaw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-igraph\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/a0/4e7134f803737aa6eebb4e5250565ace0e2599659e22be7f7eba520ff017/python-igraph-0.7.1.post6.tar.gz (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 9.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-igraph\n",
            "  Building wheel for python-igraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/d6/02/34eebae97e25f5b87d60f4c0687e00523e3f244fa41bc3f4a7\n",
            "Successfully built python-igraph\n",
            "Installing collected packages: python-igraph\n",
            "Successfully installed python-igraph-0.7.1.post6\n",
            "Requirement already satisfied: powerlaw in /usr/local/lib/python3.6/dist-packages (1.4.6)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from powerlaw) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from powerlaw) (1.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from powerlaw) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from powerlaw) (1.16.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->powerlaw) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->powerlaw) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzuIHriVyBqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import numpy as np\n",
        "from sklearn.manifold import SpectralEmbedding\n",
        "import torch.optim as optim\n",
        "import warnings\n",
        "import scipy.sparse as sp\n",
        "from pprint import pprint\n",
        "from graph_stat import *\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT4AhEQQpgOv",
        "colab_type": "code",
        "outputId": "7f025ea2-05f2-465a-9b41-0382158ae95e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "a = np.array([\n",
        "    [0, 1, 0],\n",
        "    [1, 0, 0],\n",
        "    [0, 0, 0]\n",
        "])\n",
        "compute_graph_statistics(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LCC': 2,\n",
              " 'assortativity': nan,\n",
              " 'claw_count': 0.0,\n",
              " 'clustering_coefficient': 0.0,\n",
              " 'cpl': 1.0,\n",
              " 'd': 0.6666666666666666,\n",
              " 'd_max': 1,\n",
              " 'd_min': 0,\n",
              " 'edge_num': 1,\n",
              " 'gini': -0.33333333333333326,\n",
              " 'n_components': 2,\n",
              " 'node_num': 3,\n",
              " 'power_law_exp': inf,\n",
              " 'rel_edge_distr_entropy': 0.6308387341996874,\n",
              " 'square_count': 0,\n",
              " 'triangle_count': 0,\n",
              " 'wedge_count': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjmo9SNCmCrB",
        "colab_type": "code",
        "outputId": "f92d6c66-cbe2-469e-d6cc-3de3ecd65b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "a = torch.Tensor([1]).cuda()\n",
        "print(a)\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLRf3Q-_yBqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import copy\n",
        "\n",
        "def show_graph(adj, bd, thresh=.5):\n",
        "    if not isinstance(adj, np.ndarray):\n",
        "        adj_ = adj.data.cpu().numpy()\n",
        "    else:\n",
        "        adj_ = copy.deepcopy(adj)\n",
        "    # adj_ = adj_ / np.max(adj_)\n",
        "    rows, cols = np.where(adj_ > thresh)\n",
        "    if rows.size == 0:\n",
        "        print('empty graph to print')\n",
        "        return\n",
        "    edges = zip(rows.tolist(), cols.tolist())\n",
        "    gr = nx.Graph()\n",
        "    gr.add_edges_from(edges)\n",
        "    nx.draw(gr, node_size=30)\n",
        "    plt.show()\n",
        "    a = nx.to_numpy_array(gr)\n",
        "    d = compute_graph_statistics(a)\n",
        "    pprint(d)\n",
        "    diff_d = {}\n",
        "    \n",
        "\n",
        "    for k in list(d.keys()):\n",
        "        diff_d[k] = abs(d[k] - bd[k])\n",
        "    print((diff_d.keys()))\n",
        "    print((diff_d.values()))\n",
        "    return diff_d\n",
        "    \n",
        "\n",
        "# def show_graph(adj, base_adj=None, remove_isolated=False):\n",
        "#     if not isinstance(adj, np.ndarray):\n",
        "#         adj_ = adj.data.cpu().numpy()\n",
        "#     else:\n",
        "#         adj_ = copy.deepcopy(adj)\n",
        "    \n",
        "#     adj_ -= np.diag(np.diag(adj_))\n",
        "   \n",
        "#     gr = nx.from_numpy_array(adj_)\n",
        "#     assert((adj_ == adj_.T).all())\n",
        "#     if remove_isolated:\n",
        "#         gr.remove_nodes_from(list(nx.isolates(gr)))\n",
        "#     nx.draw(gr, node_size=10)\n",
        "#     plt.title('gen')\n",
        "#     plt.show()\n",
        "#     d = compute_graph_statistics(adj_)\n",
        "#     pprint(d)\n",
        "    \n",
        "#     if base_adj is not None:\n",
        "#         base_gr = nx.from_numpy_array(base_adj)\n",
        "#         nx.draw(base_gr, node_size=10)\n",
        "#         plt.title('base')\n",
        "#         plt.show()\n",
        "#         bd = compute_graph_statistics(base_adj)\n",
        "#         diff_d = {}\n",
        "#         for k in list(d.keys()):\n",
        "#             diff_d[k] = round(abs(d[k] - bd[k]), 4)\n",
        "#         print(diff_d.keys())\n",
        "#         print(diff_d.values())\n",
        "\n",
        "\n",
        "# def show_graph(adj, thresh=.5):\n",
        "#     if not isinstance(adj, np.ndarray):\n",
        "#         adj_ = adj.data.cpu().numpy()\n",
        "#     else:\n",
        "#         adj_ = copy.deepcopy(adj)\n",
        "#     # adj_ = adj_ / np.max(adj_)\n",
        "#     rows, cols = np.where(adj_ > thresh)\n",
        "#     if rows.size == 0:\n",
        "#         print('empty graph to print')\n",
        "#         return\n",
        "#     edges = zip(rows.tolist(), cols.tolist())\n",
        "#     gr = nx.Graph()\n",
        "#     gr.add_edges_from(edges)\n",
        "#     nx.draw(gr, node_size=30)\n",
        "#     plt.show()\n",
        "#     a = nx.to_numpy_array(gr)\n",
        "#     pprint(compute_graph_statistics(a))\n",
        "    \n",
        "        \n",
        "def make_symmetric(m):\n",
        "    m_ = torch.transpose(m)\n",
        "    w = torch.max(m_, m_.T)\n",
        "    return w\n",
        "\n",
        "\n",
        "def make_adj(x, n):\n",
        "    res = torch.zeros(n, n).cuda()\n",
        "    i = 0\n",
        "    for r in range(1, n):\n",
        "        for c in range(r, n):\n",
        "            res[r, c] = x[i]\n",
        "            res[c, r] = res[r, c]\n",
        "            i += 1\n",
        "    return res\n",
        "\n",
        "\n",
        "def cat_attr(x, attr_vec):\n",
        "    if attr_vec is None:\n",
        "        return x\n",
        "    attr_mat = attr_vec.repeat(x.size()[0], 1)\n",
        "    x = torch.cat([x, attr_mat], dim=1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_spectral_embedding(adj, d):\n",
        "    \"\"\"\n",
        "    Given adj is N*N, return its feature mat N*D, D is fixed in model\n",
        "    :param adj:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    \n",
        "    adj_ = adj.data.cpu().numpy()\n",
        "    emb = SpectralEmbedding(n_components=d)\n",
        "    res = emb.fit_transform(adj_)\n",
        "    x = torch.from_numpy(res).float().cuda()\n",
        "    '''\n",
        "    x=torch.eye(adj.size()[0]).cuda()\n",
        "    '''\n",
        "    return x\n",
        "\n",
        "'''\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    mx = mx.data.cpu().numpy()\n",
        "    mx += sp.eye(mx.shape[0])\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return torch.from_numpy(mx).float().cuda()\n",
        "'''\n",
        "def normalize(adj):\n",
        "    adj = adj.data.cpu().numpy()\n",
        "    adj_ = adj + np.eye(adj.shape[0])\n",
        "    rowsum = np.array(adj_.sum(1))\n",
        "    degree_mat_inv_sqrt = np.diag(np.power(rowsum, -0.5).flatten())\n",
        "    degree_mat_sqrt = np.diag(np.power(rowsum, 0.5).flatten())\n",
        "    adj_normalized = degree_mat_inv_sqrt.dot(adj_).dot(degree_mat_sqrt)\n",
        "    return torch.from_numpy(adj_normalized).float().cuda()\n",
        "\n",
        "def preprocess_graph(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    adj_ = adj + sp.eye(adj.shape[0])\n",
        "    rowsum = np.array(adj_.sum(1))\n",
        "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "    return sparse_to_tuple(adj_normalized)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR7qg3rF_Z5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pprint import pprint\n",
        "# from collections import defaultdict\n",
        "\n",
        "# NODE_FILE = 'node.dat'\n",
        "# LINK_FILE = 'link.dat'\n",
        "# LABEL_FILE = 'label.dat'\n",
        "# DATA_DIR = 'data'\n",
        "# mat_names = []\n",
        "# adj_mats = []\n",
        "# id_maps = []\n",
        "\n",
        "# cnt = 0\n",
        "# for folder in os.listdir(DATA_DIR):\n",
        "#     cnt += 1\n",
        "#     if cnt > 100:\n",
        "#         break\n",
        "#     mat_names.append(folder)\n",
        "#     id_to_item = {}\n",
        "#     with open(os.path.join(DATA_DIR, folder, NODE_FILE), 'r') as f:\n",
        "#         for i, line in enumerate(f):\n",
        "#             cells = line.split('\\t')\n",
        "#             id_to_item[i] = cells[0]\n",
        "\n",
        "#     all_items = set(id_to_item.values())\n",
        "#     all_ids = set(id_to_item.keys())\n",
        "        \n",
        "#     links = defaultdict(set)\n",
        "#     with open(os.path.join(DATA_DIR, folder, LINK_FILE), 'r') as f:\n",
        "#         for line in f:\n",
        "#             cells = line.rstrip('\\n').split('\\t')\n",
        "#             from_id = int(cells[0])\n",
        "#             to_id = int(cells[1])\n",
        "#             if from_id in all_ids and to_id in all_ids:\n",
        "#                 links[from_id].add(to_id)\n",
        "    \n",
        "#     N = len(all_ids)\n",
        "#     adj = np.zeros((N, N))\n",
        "#     for from_id in range(N):\n",
        "#         for to_id in links[i]:\n",
        "#             adj[from_id, to_id] = 1\n",
        "#             adj[to_id, from_id] = 1\n",
        "#     id_map = [id_to_item[i] for i in range(N)]\n",
        "    \n",
        "#     adj_mats.append(adj)\n",
        "#     id_maps.append(id_map)\n",
        "    \n",
        "\n",
        "# t_adj_mats = [torch.from_numpy(m).float() for m in adj_mats]\n",
        "# torch.manual_seed(0)\n",
        "# t_attr_vecs = torch.randn(len(t_adj_mats), 8)\n",
        "    \n",
        "\n",
        "# for i in range(3):\n",
        "#     print('No:', i, mat_names[i])\n",
        "#     print('Adj Mat', adj_mats[i].shape)\n",
        "#     print(adj_mats[i])\n",
        "#     show_graph(adj_mats[i])\n",
        "#     print('# of nodes', len(id_maps[i]))\n",
        "#     print('# of links', np.count_nonzero(adj_mats[i]) // 2)\n",
        "#     print('Item names', id_maps[i])\n",
        "#     print('Attr vec', t_attr_vecs[i, :])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfooBFdO5RL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pprint import pprint\n",
        "# from collections import defaultdict\n",
        "\n",
        "# NODE_FILE = 'sub.node.dat'\n",
        "# LINK_FILE = 'sub.link.dat'\n",
        "# LABEL_FILE = 'sub.label.dat'\n",
        "\n",
        "# id_to_author = {}\n",
        "# id_to_fv = {}\n",
        "# with open(NODE_FILE, 'r') as f:\n",
        "#     for i, line in enumerate(f):\n",
        "#         cells = line.split('\\t')\n",
        "#         if cells[1] == '1':\n",
        "#             id_to_author[i] = cells[0]\n",
        "#             id_to_fv[i] = np.asarray([float(a) for a in cells[-1].split(',')])\n",
        "# all_authors = set(id_to_author.values())\n",
        "# all_ids = set(id_to_author.keys())\n",
        "\n",
        "# sg_to_ids = defaultdict(list)\n",
        "# author_to_id = {v: k for k, v in id_to_author.items()}\n",
        "# with open(LABEL_FILE, 'r') as f:\n",
        "#     for line in f:\n",
        "#         cells = line.rstrip('\\n').split('\\t')\n",
        "#         if cells[1] == '1' and cells[0] in all_authors:\n",
        "#             sg_to_ids[int(cells[2])].append(author_to_id[cells[0]])\n",
        "# for k, v in sg_to_ids.items():\n",
        "#     v.sort()\n",
        "    \n",
        "# links = defaultdict(set)\n",
        "# with open(LINK_FILE, 'r') as f:\n",
        "#     for line in f:\n",
        "#         cells = line.rstrip('\\n').split('\\t')\n",
        "#         from_id = int(cells[0])\n",
        "#         to_id = int(cells[1])\n",
        "#         if from_id in all_ids and to_id in all_ids:\n",
        "#             links[from_id].add(to_id)\n",
        "# print('total links', sum(len(s) for s in links.values()))\n",
        "\n",
        "# adj_mats = []\n",
        "# id_maps = []\n",
        "# author_maps = []\n",
        "# feature_mats = []\n",
        "# M = len(id_to_fv[15055])\n",
        "# for sg, ids in sg_to_ids.items():\n",
        "#     N = len(ids)\n",
        "#     cur_ids = set(ids)\n",
        "#     adj_mat = np.zeros((N, N))\n",
        "#     feature_mat = np.zeros((N, M))\n",
        "#     row_map = {id: i for i, id in enumerate(ids)}\n",
        "#     for id in ids:\n",
        "#         for to_id in links[id]:\n",
        "#             if to_id not in cur_ids:\n",
        "#                 continue\n",
        "#             r = row_map[id]\n",
        "#             c = row_map[to_id]\n",
        "#             adj_mat[r, c] = 1\n",
        "#             adj_mat[c, r] = 1\n",
        "#             feature_mat[r, :] = id_to_fv[id]\n",
        "#     adj_mats.append(adj_mat)\n",
        "#     id_maps.append(ids)\n",
        "#     author_maps.append([id_to_author[id] for id in ids])\n",
        "#     feature_mats.append(feature_mat)\n",
        "\n",
        "# for i in range(len(adj_mats)):\n",
        "#     print('Sub Group:', i)\n",
        "#     print('Adj Mat', adj_mats[i].shape)\n",
        "#     print(adj_mats[i])\n",
        "#     print('Feature Mat', feature_mats[i].shape)\n",
        "#     print(feature_mats[i])\n",
        "#     print('# of nodes', len(id_maps[i]))\n",
        "#     print('# of links', np.count_nonzero(adj_mats[i]) // 2)\n",
        "#     print('Author IDs', id_maps[i])\n",
        "#     print('Author Names', author_maps[i])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77cXgy9wy7FU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keep_topk_conns(adj, k=3):\n",
        "    g = nx.from_numpy_array(adj)\n",
        "    to_removes = [cp for cp in sorted(nx.connected_components(g), key=len)][:-k]\n",
        "    for cp in to_removes:\n",
        "        g.remove_nodes_from(cp)\n",
        "    adj = nx.to_numpy_array(g)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def remove_small_conns(adj, keep_min_conn=4):\n",
        "    g = nx.from_numpy_array(adj)\n",
        "    for cp in list(nx.algorithms.components.connected_components(g)):\n",
        "        if len(cp) < keep_min_conn:\n",
        "            g.remove_nodes_from(cp)\n",
        "    adj = nx.to_numpy_array(g)\n",
        "    return adj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SnP-jQeyh7U",
        "colab_type": "code",
        "outputId": "dc9ed301-f613-4faa-c4a4-f18c1a31bcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1448
        }
      },
      "source": [
        "# script for loading NWE dblp\n",
        "# folder structure\n",
        "# - this.ipynb\n",
        "# - $DATA_DIR - *.txt\n",
        "\n",
        "from pprint import pprint\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "NODE_FILE = 'node.dat'\n",
        "LINK_FILE = 'link.dat'\n",
        "LABEL_FILE = 'label.dat'\n",
        "ATTR_FILE = 'attribute.dat'\n",
        "DATA_DIR = 'PGG_dblp'\n",
        "mat_names = []  # e.g. GSE_2304\n",
        "adj_mats = []  # essential data, type: list(np.ndarray)\n",
        "attr_vecs = [] # essential data, type: list(np.ndarray)\n",
        "id_maps = []  # map index to gene name if you need\n",
        "\n",
        "for f in os.listdir(DATA_DIR):\n",
        "    if not f.startswith(('nodes', 'links', 'attrs')):\n",
        "        continue\n",
        "    else:\n",
        "        mat_names.append('_'.join(f.split('.')[0].split('_')[1:]))\n",
        "mat_names = sorted([it for it in set(mat_names)])\n",
        "print(mat_names)\n",
        "print('Test length', len(mat_names))\n",
        "for mat_name in mat_names:\n",
        "    node_file = 'nodes_' + mat_name + '.txt'\n",
        "    link_file = 'links_' + mat_name + '.txt'\n",
        "    attr_file  = 'attrs_' + mat_name + '.txt'\n",
        "    node_file_path = os.path.join(DATA_DIR, node_file)\n",
        "    link_file_path = os.path.join(DATA_DIR, link_file)\n",
        "    attr_file_path = os.path.join(DATA_DIR, attr_file)\n",
        "    \n",
        "    id_to_item = {}\n",
        "    with open(node_file_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            author = line.rstrip('\\n')\n",
        "            id_to_item[i] = author\n",
        "    all_ids = set(id_to_item.keys())\n",
        "    \n",
        "    with open(attr_file_path, 'r') as f:\n",
        "        attr_vec = np.loadtxt(f).T.flatten()\n",
        "        attr_vecs.append(attr_vec)\n",
        "    \n",
        "    links = defaultdict(set)\n",
        "    with open(link_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            cells = line.rstrip('\\n').split(',')\n",
        "            from_id = int(cells[0])\n",
        "            to_id = int(cells[1])\n",
        "            if from_id in all_ids and to_id in all_ids:\n",
        "                links[from_id].add(to_id)\n",
        "    \n",
        "    N = len(all_ids)\n",
        "    adj = np.zeros((N, N))\n",
        "    for from_id in range(N):\n",
        "        for to_id in links[from_id]:\n",
        "            adj[from_id, to_id] = 1\n",
        "            adj[to_id, from_id] = 1\n",
        "            \n",
        "    adj -= np.diag(np.diag(adj))\n",
        "    id_map = [id_to_item[i] for i in range(N)]\n",
        "    \n",
        "    \n",
        "    # Remove small component\n",
        "    # adj = remove_small_conns(adj, keep_min_conn=4)\n",
        "    \n",
        "    # Keep large component \n",
        "    adj = keep_topk_conns(adj, k=1)\n",
        "    adj_mats.append(adj)\n",
        "    id_maps.append(id_map)\n",
        "    \n",
        "    if int(np.sum(adj)) == 0:\n",
        "        adj_mats.pop(-1)\n",
        "        id_maps.pop(-1)\n",
        "        mat_names.pop(-1)\n",
        "        attr_vecs.pop(-1)\n",
        "                \n",
        "        \n",
        "        \n",
        "# print some samples\n",
        "for i in range(5):\n",
        "    print('No:', i, mat_names[i])\n",
        "    print('Adj Mat', adj_mats[i].shape)\n",
        "    print(adj_mats[i])\n",
        "    print('# of nodes', len(id_maps[i]))\n",
        "    print('# of links', np.count_nonzero(adj_mats[i]) // 2)\n",
        "    print('Atrribute vector', attr_vecs[i].shape)\n",
        "    print(attr_vecs[i])\n",
        "    print('Item names', id_maps[i])\n",
        "    print('Components')\n",
        "    print(list(nx.algorithms.components.connected_components(nx.from_numpy_array(adj_mats[i]))))\n",
        "    \n",
        "train_adj_mats = adj_mats[:int(len(adj_mats) * .8)]\n",
        "test_adj_mats = adj_mats[int(len(adj_mats) * .8):]\n",
        "train_attr_vecs = attr_vecs[:int(len(attr_vecs) * .8)]\n",
        "test_attr_vecs = attr_vecs[int(len(attr_vecs) * .8):]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ckim_00-09_0-10', 'ckim_00-09_10-30', 'ckim_00-09_30+', 'ckim_10-19_0-10', 'ckim_10-19_10-30', 'ckim_10-19_30+', 'ckim_90-99_0-10', 'ckim_90-99_10-30', 'ckim_90-99_30+', 'icdm_00-09_0-10', 'icdm_00-09_10-30', 'icdm_00-09_30+', 'icdm_10-19_0-10', 'icdm_10-19_10-30', 'icdm_10-19_30+', 'icdm_90-99_0-10', 'icdm_90-99_10-30', 'icdm_90-99_30+', 'icml_00-09_0-10', 'icml_00-09_10-30', 'icml_00-09_30+', 'icml_10-19_0-10', 'icml_10-19_10-30', 'icml_10-19_30+', 'icml_90-99_0-10', 'icml_90-99_10-30', 'icml_90-99_30+', 'kdd_00-09_0-10', 'kdd_00-09_10-30', 'kdd_00-09_30+', 'kdd_10-19_0-10', 'kdd_10-19_10-30', 'kdd_10-19_30+', 'kdd_90-99_0-10', 'kdd_90-99_10-30', 'kdd_90-99_30+', 'nips_00-09_0-10', 'nips_00-09_10-30', 'nips_00-09_30+', 'nips_10-19_0-10', 'nips_10-19_10-30', 'nips_10-19_30+', 'nips_90-99_0-10', 'nips_90-99_10-30', 'nips_90-99_30+', 'sigir_00-09_0-10', 'sigir_00-09_10-30', 'sigir_00-09_30+', 'sigir_10-19_0-10', 'sigir_10-19_10-30', 'sigir_10-19_30+', 'sigir_90-99_0-10', 'sigir_90-99_10-30', 'sigir_90-99_30+', 'sigmod_00-09_0-10', 'sigmod_00-09_10-30', 'sigmod_00-09_30+', 'sigmod_10-19_0-10', 'sigmod_10-19_10-30', 'sigmod_10-19_30+', 'sigmod_90-99_0-10', 'sigmod_90-99_10-30', 'sigmod_90-99_30+', 'vldb_00-09_0-10', 'vldb_00-09_10-30', 'vldb_00-09_30+', 'vldb_10-19_0-10', 'vldb_10-19_10-30', 'vldb_10-19_30+', 'vldb_90-99_0-10', 'vldb_90-99_10-30', 'vldb_90-99_30+']\n",
            "Test length 72\n",
            "No: 0 ckim_00-09_0-10\n",
            "Adj Mat (3, 3)\n",
            "[[0. 1. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n",
            "# of nodes 20\n",
            "# of links 2\n",
            "Atrribute vector (10,)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 2. 1.]\n",
            "Item names ['Ruijie Guo', 'Devanand Ravindran', 'Vaughan R. Shanks', 'Julia Luxenburger', 'Ao Feng', 'Wahyu Wibowo', 'Jason Chaffee', 'Alireza Mokhtaripour', 'Karane Vieira', 'Giorgos Margaritis', 'Nasreen AbdulJaleel', 'Bin Lan', 'Cheng-Yue Chang', 'Jeff Pasternack', 'Mingjie Zhu', 'Ahu Sieg', 'Sairam Gurajada', 'Kashif Riaz', 'Stephen Cronen-Townsend', 'Christian Kohlschütter']\n",
            "Components\n",
            "[{0, 1, 2}]\n",
            "No: 1 ckim_00-09_10-30\n",
            "Adj Mat (7, 7)\n",
            "[[0. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]]\n",
            "# of nodes 80\n",
            "# of links 6\n",
            "Atrribute vector (10,)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 2. 2.]\n",
            "Item names ['Falk Brauer', 'James J. Gardner', 'Paul-Alexandru Chirita', 'Moshe Fresko', 'Jangwon Seo', 'Wenhui Liao', 'Chong Long', 'Bhuvan Bamba', 'Leah S. Larkey', 'Hao-Ping Hung', 'Li Zhuang', 'Dongmei Jia', 'Stefan Büttcher', 'Derrick Coetzee', 'Ding Zhou', 'Eduarda Mendes Rodrigues', 'Oisín Boydell', 'Pawel Jurczyk', 'Paavo Arvola', 'Rebecca Cathey', 'Benjamin Rosenfeld', 'Hongliang Fei', 'Linhong Zhu', 'Maryam Karimzadehgan', 'Lixin Shi', 'Chun Tang', 'Kerstin Bischoff', 'Lilian Harada', 'Nicholas Lester', 'Le Zhao', 'James W. Cooper', 'Bodo Billerbeck', 'Bingjun Sun', 'Qiankun Zhao', 'Eric C. Jensen', 'Shui-Lung Chuang', 'Munirathnam Srikanth', 'Xuehua Shen', 'Beverly Yang', 'Xiaoguang Qi', 'Andrew Hickl', 'Eric J. Glover', 'Marina Barsky', 'Timothy G. Armstrong', 'Zheng-Yu Niu', 'Chih-Chin Liu', 'Lingpeng Yang', 'Asad B. Sayeed', 'Abhishek Mukherji', 'Susan Price', 'Sarah Zelikovitz', 'Changzhou Wang', 'Hyun Duk Kim', 'Chang-Hung Lee', 'Jack G. Conrad', 'Thomas R. Lynam', 'David N. Milne', 'Changqing Li', 'Viet Ha-Thuc', 'David Liben-Nowell', 'Zhicheng Dou', 'Caimei Lu', 'Desmond Elliott', 'Joong Hyuk Chang', 'Ramakrishna Varadarajan', 'Dongmei Ren', 'Prasan Roy', 'Xiaoyong Liu', 'Ramesh Nallapati', 'Vuk Ercegovac', 'Paul Ogilvie', 'Fabien Duchateau', 'Flavio Figueiredo', 'Like Gao', 'Xianpei Han', 'Kamal Nigam', 'Jiwoon Jeon', 'Stefanos Souldatos', 'Einat Amitay', 'Cai-Nicolas Ziegler']\n",
            "Components\n",
            "[{0, 1, 2, 3, 4, 5, 6}]\n",
            "No: 2 ckim_00-09_30+\n",
            "Adj Mat (76, 76)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "# of nodes 194\n",
            "# of links 84\n",
            "Atrribute vector (10,)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 2. 3.]\n",
            "Item names ['Fabian Abel', 'Shuming Shi', 'Edgar Meij', 'Ronny Lempel', 'Yi Ma', 'Valentin Jijkoun', 'Emine Yilmaz', 'Vana Kalogeraki', 'Ben He', 'Sebastian Michel', 'Doron Rotem', 'Nick Koudas', 'Benjamin Piwowarski', 'Sujith Ravi', 'Leif Azzopardi', 'Xiang Lian', 'Luis Gravano', 'Azadeh Shakery', 'Claudia Hauff', 'Georgios Paltoglou', 'Hao Ma', 'Pável Calado', 'Jiaheng Lu', 'Petteri Nurmi', 'Marius Pasca', 'Gilad Mishne', 'Lei Wu', 'Xiaoyan Li', 'Hwanjo Yu', 'Ali Dasdan', 'Chen Chen', 'Ming Zhong', 'William Webber', 'Bin Wu', 'Yangqiu Song', 'Kevyn Collins-Thompson', 'David Grangier', 'Jure Leskovec', 'Dimitri Theodoratos', 'Anand Ranganathan', 'Amit Anil Nanavati', 'Ning Liu', 'Tomonari Masada', 'Qiaozhu Mei', 'Karl Aberer', 'David E. Losada', 'Shixia Liu', 'Bo Long', 'Gui-Rong Xue', 'Daniel Barbará', 'Rayid Ghani', 'Jin Zhang', 'Ömer Eğecioğlu', 'Yi-Leh Wu', 'Olivier Chapelle', 'Massimo Melucci', 'Linyuan Lü', 'Wei Zhang', 'Wei Dong', 'Ranieri Baraglia', 'Jianliang Xu', 'Wookey Lee', 'Bin Liu', 'Falk Scholer', 'Rosie Jones', 'Tetsuya Sakai', 'Paolo Boldi', 'Ana Gabriela Maguitman', 'Li Ding', 'Alberto Lavelli', 'Luo Si', 'Xin Xin', 'Adriano Veloso', 'Bing Bai', 'Chia-Jung Lee', 'James Allan', 'Jing He', 'Nan Ma', 'Hua Yan', 'Jianguo Lu', 'Youngja Park', 'Donald Metzler', 'Changhu Wang', 'Ying Zhao', 'Xiaojun Wan', 'Hang Li', 'Yun Zhou', 'Kamesh Madduri', 'Dmitri Roussinov', 'Peter Bailey', 'Andrei Z. Broder', 'Robert West', 'Krisztian Balog', 'Jeremy Pickens', 'Umit Y. Ogras', 'Gang Luo', 'Jie Lu', 'Chirag Shah', 'Victor Lavrenko', 'Ryen W. White', 'Fernando Diaz', 'Songbo Tan', 'Kai-Uwe Sattler', 'Reynold Cheng', 'Leonardo C. da Rocha', 'Ning Jin', 'Craig Macdonald', 'Yang Song', 'Keke Chen', 'Hongyuan Zha', 'Sara Cohen', 'Xin Liu', 'Yue Xu', 'Dragomir R. Radev', 'Bo Wang', 'Jaap Kamps', 'Xuanhui Wang', 'Krysta M. Svore', 'Pavel Serdyukov', 'Sourav S. Bhowmick', 'Charles L. A. Clarke', 'Jeff Huang', 'Doug Downey', 'Sam Yuan Sung', 'David Fernandes', 'Furu Wei', 'Kun Lung Wu', 'Stephen E. Robertson', 'Takao Miura', 'Dingding Wang', 'Xiaoli Li', 'Victor Muntés-Mulero', 'Jun Wang', 'Javed A. Aslam', 'Carlos Ordonez', 'Christos Doulkeridis', 'Marc Najork', 'Peter D. Bruza', 'Xin Cao', 'Sheng Guo', 'Hugo Zaragoza', 'Lipyeow Lim', 'Li Ma', 'Deepak Agarwal', 'Huanhuan Cao', 'Shuang Liu', 'Shenghua Bao', 'Qiang Huang', 'Yuanhua Lv', 'Yuefeng Li', 'Qin Lv', 'Jing Bai', 'Rada Mihalcea', 'Wei Dai', 'Jimmy J. Lin', 'Katja Hose', 'James P. Callan', 'Matthew Lease', 'Fabrizio Sebastiani', 'Ben Carterette', 'Tapas Kanungo', 'Ping Luo', 'Wai Gen Yee', 'Gianluca Demartini', 'Yuqing Wu', 'Francisco M. Couto', 'Mark D. Smucker', 'Hakan Ferhatosmanoglu', 'Guoliang Li', 'Ke Sun', 'Zaiqing Nie', 'Xin Li', 'William E. Jones', 'Yi Ding', 'Ling Ma', 'Kang Liu', 'Jinyoung Kim', 'Anton Leuski', 'Bin He', 'Jaime Arguello', 'Andrea Esuli', 'Aris Anagnostopoulos', 'Bin Cao', 'Ziv Bar-Yossef', 'Hui Li', 'David Carmel', 'Anoop Singhal', 'Takahiro Hara', 'Jiun-Long Huang', 'ChengXiang Zhai', 'Filip Radlinski', 'Aleksander Kolcz', 'Dou Shen', 'Jie Peng']\n",
            "Components\n",
            "[{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75}]\n",
            "No: 3 ckim_10-19_0-10\n",
            "Adj Mat (12, 12)\n",
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "# of nodes 156\n",
            "# of links 12\n",
            "Atrribute vector (10,)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 3. 1.]\n",
            "Item names ['Cane Wing-Ki Leung', 'Sami Abu-El-Haija', 'Minh C. Phan', 'Hagit Cohen', 'Yu-Hsuan Kuo', 'Xin-Chao Xu', 'Tomasz Kusmierczyk', 'Niels Dalum Hansen', 'Meike Zehlike', 'Sairam Gurajada', 'Raíza Hanada', 'John S. Whissell', 'Avikalp Srivastava', 'Ali Braytee', 'Anton Bakhtin', 'Silvia Rota', 'Lijing Qin', 'Rudra M. Tripathy', 'Anahita Hosseini', 'Abhishek Sikchi', 'Imrul Chowdhury Anindya', 'Giuseppe Amodeo', 'Ryadh Dahimene', 'Snehasis Banerjee', 'Tuan-Anh Hoang-Vu', 'Chonggang Song', 'Henry Vieira', 'Satoshi Sanjo', 'Wenjie Pei', 'Rana Hussein', 'Alican Büyükçakir', 'Alejandro Marcos Alvarez', 'Jun-Gi Jang', 'Kerui Min', 'Matthew W. Bilotti', 'Sandro Cavallari', 'Feruz Davletov', 'Ha-Myung Park', 'Wen Chan', 'Shaosheng Cao', 'Andrey Styskin', 'Adit Krishnan', 'Chia-An Yu', 'Lance De Vine', 'Ramakrishna Bairi', 'Zongcheng Ji', 'Mengdie Zhuang', 'Maxim Zhukovskiy', 'William Lucia', 'Wang-Cheng Kang', 'Xian Teng', 'Melissa Ailem', 'Walther Neuper', 'Zhen Liao', 'Casper Worm Hansen', 'Jiongqian Liang', 'Ruben Sipos', 'Tao Yang Fu', 'Yujie Fan', 'Weiyi Xia', 'Jatin Arora', 'Konstantin Tretyakov', 'Nico Schlaefer', 'Gad Markovits', 'Swapnil Mishra', 'Amin Y. Teymorian', 'Yusra Ibrahim', 'Jiangfeng Zeng', 'Alfan Farizki Wicaksono', 'Biao Chang', 'Jasper Linmans', 'Sergio Duarte Torres', 'Qizhen Zhang', 'Kezun Zhang', 'Shiwen Cheng', 'Zhen Hai', 'Elif Aktolga', 'Xuntao Cheng', 'Myungha Jang', 'Yaogong Zhang', 'Yonathan Perez', 'Mustafa Zengin', 'Zhiyuan Cai', 'Nikita V. Spirin', 'Weihuang Huang', 'Jeffrey McGee', 'Daniele Broccolo', 'Hamed R. Bonab', 'Bilyana Taneva', 'Joel Coffman', 'Yuli Liu', 'Saiping Guan', 'Shiri Dori-Hacohen', 'Thomas Stone', 'Omar Khattab', 'Bing-Jie Sun', 'Kai-Yang Chiang', 'Maria Christoforaki', 'Shitao Zhang', 'Wanying Ding', 'Hideaki Kim', 'Boris Sharchilev', 'Matteo Catena', 'Weize Kong', 'Sumita Barahmand', 'S. Cheng', 'Zhuoyu Wei', 'Caitlin Kuhlman', 'Andrey Kustarev', 'Erdal Kuzey', 'Sarah K. Tyler', 'Kais Allab', 'Joon Hee Kim', 'Shubhra Kanti Karmaker Santu', 'Zhaochen Guo', 'Qiongkai Xu', 'Chen Chu', 'Thông T. Nguyên', 'Ridho Reinanda', 'Radityo Eko Prasojo', 'Huiyuan Chen', 'Yuling Shi', 'Thông T. Nguyễn', 'Haochen Chen', 'Sarah Masud Preum', 'Alexey Baytin', 'Seyyedeh Newsha Ghoreishi', 'Nelly Vouzoukidou', 'Kajta Hofmann', 'Afroza Sultana', 'Renqin Cai', 'Fanghua Ye', 'Maksim Tkachenko', 'Feza Baskaya', 'Tung Kieu', 'Dejian Yang', 'Tianshu Lyu', 'Chaozhuo Li', 'Yann Jacob', 'Xiaojia Pu', 'Ruicheng Zhong', 'Qiyun Zhao', 'Edoardo Galimberti', 'Tarique Siddiqui', 'Dwaipayan Roy', 'Zhung-Xun Liao', 'Namyong Park', 'Behrooz Mansouri', 'Rawia Awadallah', 'Mark Wilhelm', 'Chen Tse Tsai', 'Muhammad Ali Norozi', 'Qitian Wu', 'Ahmet Murat Ozdemiray', 'Jianglei Han', 'Razvan-Gabriel Cirstea']\n",
            "Components\n",
            "[{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}]\n",
            "No: 4 ckim_10-19_10-30\n",
            "Adj Mat (74, 74)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "# of nodes 262\n",
            "# of links 86\n",
            "Atrribute vector (10,)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 3. 2.]\n",
            "Item names ['Mohamed E. Khalefa', 'Benjamin Roth', 'Mehdi Hosseini', 'Yunlong He', 'Michael L. Wick', 'Minkyoung Kim', 'Mehdi Kargar', 'Michael Schuhmacher', 'Jiafeng Hu', 'Runwei Qiang', 'Julia Kiseleva', 'Qiyue Yin', 'Yong-Yeon Jo', 'Suppawong Tuarob', 'Ian Wood', 'Jonathan Gemmell', 'Einat Minkov', 'Jiaming Shen', 'Kyosuke Nishida', 'Guoliang He', 'Mansurul Bhuiyan', 'Taiki Miyanishi', 'Luca Bonomi', 'Yelong Shen', 'Abhijith Kashyap', 'Po-Sen Huang', 'Victor W. Chu', 'Wenyi Huang', 'Andrey Gubichev', 'Romain Deveaud', 'Zhiyuan Cheng', 'Carolina Bonacic', 'Jeffrey Pound', 'Yunzhong Liu', 'Yafang Wang', 'Daifeng Li', 'Yuto Yamaguchi', 'Jiangtao Yin', 'Pritam Gundecha', 'Ruey-Cheng Chen', 'George D. Montanez', 'Damien Lefortier', 'Yin Zhu', 'Kateryna Tymoshenko', 'Chenxi Qiu', 'Aditya Pal', 'Jinfeng Rao', 'Alan Medlar', 'Seyyed Hadi Hashemi', 'Henning Koehler', 'Tianbing Xu', 'Suhas Ranganath', 'Jia-Dong Zhang', 'Jyun-Yu Jiang', 'Ioannis Arapakis', 'Zhen Yue', 'Dinusha Vatsalan', 'Elad Kravi', 'Maksims Volkovs', 'Samaneh Moghaddam', 'Takanori Hayashi', 'Fajie Yuan', 'Joyce Jiyoung Whang', 'Dae Hoon Park', 'Aleksandr Chuklin', 'Zheng Lin', 'Krishna Yeswanth Kamath', 'Huizhong Duan', 'Michael Völske', 'Yosi Mass', 'Ahmed Hassan Awadallah', 'Ariyam Das', 'Laure Soulier', 'Agus Trisnajaya Kwee', 'Ricardo Campos', 'Marek Ciglan', 'Xiaobing Xue', 'Duck-Ho Bae', 'Eugene Kharitonov', 'Gabriele Tolomei', 'Weiguo Zheng', 'Mingjie Qian', 'Feifan Fan', 'Abdigani Diriye', 'Laura Dietz', 'Markus Rokicki', 'Xiaoxue Zhao', 'Anne Schuth', 'Haishuai Wang', 'Mohammad Aliannejadi', 'Seongsoon Kim', 'Zhuoren Jiang', 'Maryam Karimzadehgan', 'Merih Seran Uysal', 'Mostafa Keikha', 'M-Dyaa Albakour', 'Qifan Wang', 'Lars Borin', 'Massimo Nicosia', 'Jiajia Huang', 'Fiana Raiber', 'Guoqiong Liao', 'Ziawasch Abedjan', 'Quanzhi Li', 'Sérgio D. Canuto', 'Shizhu He', 'Liang Pang', 'Simon Knight', 'Hossein Soleimani', 'Kan Ren', 'Jarana Manotumruksa', 'Sha Li', 'Abhishek Mukherji', 'George Valkanas', 'Hyun Duk Kim', 'Jan Vosecky', 'Kumaripaba Athukorala', 'Pengcheng Yin', 'Jae Hyun Park', 'Dhruv Mahajan', 'Christophe Van Gysel', 'Jialong Han', 'Cheng Cao', 'Shu Huang', 'Gaurav Baruah', 'Diego Ceccarelli', 'Yu Rong', 'Noriaki Kawamae', 'Ilaria Bordino', 'Gianni Amati', 'Henning Köhler', 'Bhaskar Mitra', 'Nitin Jindal', 'Marc Bron', 'Stewart Whiting', 'Nicole Bidoit', 'Kaiqi Zhao', 'Tuan A. Tran', 'Mijung Kim', 'Chenyan Xiong', 'Mahashweta Das', 'Jaeho Choi', 'Yuhao Yang', 'NhatHai Phan', 'Shangsong Liang', 'Rishabh Mehrotra', 'Arturas Mazeika', 'Erheng Zhong', 'Anjie Fang', 'Xinsheng Li', 'Chen Ma', 'Chih-Ya Shen', 'Dong-Kyu Chae', 'Tom Kenter', 'Marina Drosou', 'Anagha Kulkarni', 'Christoph Böhm', 'Mohamed Yahya', 'Yiwei Wang', 'Chong Peng', 'Hosein Azarbonyad', 'Maram Hasanain', 'Mo Zhou', 'Xiaohui Yan', 'Le Zhao', 'Melanie Herschel', 'Joseph J. Pfeiffer', 'Martin Szummer', 'Takuya Akiba', 'Xueke Xu', 'Veli Bicer', 'Felipe Viegas', 'Tianyi Lin', 'Olivier Van Laere', 'Leonid Boytsov', 'Ilya Markov', 'Nadav Golbandi', 'Tarique Anwar', 'Bin Tong', 'Roberto Mirizzi', 'Janette Lehmann', 'Xinhui Tu', 'Jiwoon Ha', 'Yuhao Zhang', 'Chee Wee Leong', 'Atsushi Miyauchi', 'Zi Yang', 'Won-Seok Hwang', 'Peifeng Yin', 'Huizhi Liang', 'Shenghua Liu', 'Chengyao Chen', 'Noa Avigdor-Elgrabli', 'Rianne Kaptein', 'Yiyang Li', 'Ximing Li', 'Xiaofei Zhu', 'Behzad Golshan', 'Guohua Liang', 'Honglei Guo', 'Silviu Maniu', 'Bei Shi', 'Miao Fan', 'Mossaab Bagdouri', 'Jinfei Liu', 'Kunwoo Park', 'Harrie Oosterhuis', 'Shady Elbassuoni', 'Zhao Kang', 'Colin Wilkie', 'Hongliang Fei', 'Jianming Lv', 'Nut Limsopatham', 'Xitong Liu', 'Baichuan Zhang', 'Luis Galárraga', 'Xufei Wang', 'Donghyuk Shin', 'Xiaomo Liu', 'Meng Fang', 'Johannes Hoffart', 'Jiaul H. Paik', 'Alessandro Sordoni', 'Avirup Sil', 'Alexey Drutsa', 'Xiaoxiao Shi', 'Henning Wachsmuth', 'Changsha Ma', 'Miika Hannula', 'Fangzhao Wu', 'Marina Danilevsky', 'Stephan Seufert', 'Luchen Tan', 'Luke K. McDowell', 'Panagiotis Liakos', 'Ou Jin', 'John Foley', 'Jiguang Liang', 'Baichuan Li', 'Xilun Chen', 'Chenliang Li', 'Yeyun Gong', 'Hancheng Ge', 'Peng Bao', 'Javid Ebrahimi', 'Vinay Deolalikar', 'Anqi Cui', 'Zhicheng Dou', 'Yang Bao', 'Ethem F. Can', 'Royi Ronen', 'Guy Feigenblat', 'Lars Kolb', 'Yuanyuan Zhu', 'Boxiang Dong', 'Muhammad Anis Uddin Nasir', 'D.J. Maxwell', 'Marc-Allen Cartright', 'Zongyang Ma', 'Joshua V. Dillon', 'Abir De', 'Behrooz Omidvar-Tehrani']\n",
            "Components\n",
            "[{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFYZdyIBxvm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def show_graph(adj, base_adj=None, remove_isolated=True):\n",
        "    if not isinstance(adj, np.ndarray):\n",
        "        adj_ = adj.data.cpu().numpy()\n",
        "    else:\n",
        "        adj_ = copy.deepcopy(adj)\n",
        "    \n",
        "    adj_ -= np.diag(np.diag(adj_))\n",
        "   \n",
        "    gr = nx.from_numpy_array(adj_)\n",
        "    assert((adj_ == adj_.T).all())\n",
        "    if remove_isolated:\n",
        "        gr.remove_nodes_from(list(nx.isolates(gr)))\n",
        "    nx.draw(gr, node_size=10)\n",
        "    plt.title('gen')\n",
        "    plt.show()\n",
        "    \n",
        "    d = compute_graph_statistics(adj_)\n",
        "    pprint(d)\n",
        "    \n",
        "    if base_adj is not None:\n",
        "        base_gr = nx.from_numpy_array(base_adj)\n",
        "        nx.draw(base_gr, node_size=10)\n",
        "        plt.title('base')\n",
        "        plt.show()\n",
        "        bd = compute_graph_statistics(base_adj)\n",
        "        diff_d = {}\n",
        "        for k in list(d.keys()):\n",
        "            diff_d[k] = round(abs(d[k] - bd[k]), 4)\n",
        "        print(diff_d.keys())\n",
        "        print(diff_d.values())\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G6jPSY1zYht",
        "colab_type": "code",
        "outputId": "26d2d2e0-7363-42a8-cac1-3fe0120f777f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1889
        }
      },
      "source": [
        "print(len(train_adj_mats))\n",
        "print(len(test_adj_mats))\n",
        "print(len(adj_mats))\n",
        "\n",
        "for i in range(3):\n",
        "    a = train_adj_mats[i]\n",
        "    show_graph(a) # There should be a bds[i]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52\n",
            "14\n",
            "66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAE+CAYAAADyPXUxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFv5/w+vmAMX3E6nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'LCC': 3,\n",
            " 'assortativity': -1.0,\n",
            " 'claw_count': 0.0,\n",
            " 'clustering_coefficient': 0.0,\n",
            " 'cpl': 1.3333333333333333,\n",
            " 'd': 1.3333333333333333,\n",
            " 'd_max': 2.0,\n",
            " 'd_min': 1.0,\n",
            " 'edge_num': 2,\n",
            " 'gini': -0.4999999999999999,\n",
            " 'n_components': 1,\n",
            " 'node_num': 3,\n",
            " 'power_law_exp': 5.328085122666891,\n",
            " 'rel_edge_distr_entropy': 0.946326365259516,\n",
            " 'square_count': 0,\n",
            " 'triangle_count': 0,\n",
            " 'wedge_count': 1.0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAE+CAYAAADyPXUxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNcvFswAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'LCC': 7,\n",
            " 'assortativity': -0.6666666666666661,\n",
            " 'claw_count': 1.0,\n",
            " 'clustering_coefficient': 0.0,\n",
            " 'cpl': 2.4761904761904763,\n",
            " 'd': 1.7142857142857142,\n",
            " 'd_max': 3.0,\n",
            " 'd_min': 1.0,\n",
            " 'edge_num': 6,\n",
            " 'gini': -0.0714285714285714,\n",
            " 'n_components': 1,\n",
            " 'node_num': 7,\n",
            " 'power_law_exp': 3.2026058631088743,\n",
            " 'rel_edge_distr_entropy': 0.9577120798337921,\n",
            " 'square_count': 0,\n",
            " 'triangle_count': 0,\n",
            " 'wedge_count': 6.0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAE+CAYAAADyPXUxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'LCC': 76,\n",
            " 'assortativity': -0.20363675031901315,\n",
            " 'claw_count': 249.0,\n",
            " 'clustering_coefficient': 0.06,\n",
            " 'cpl': 6.415087719298246,\n",
            " 'd': 2.210526315789474,\n",
            " 'd_max': 10.0,\n",
            " 'd_min': 1.0,\n",
            " 'edge_num': 84,\n",
            " 'gini': 0.33286340852130336,\n",
            " 'n_components': 1,\n",
            " 'node_num': 76,\n",
            " 'power_law_exp': 2.7351216464482864,\n",
            " 'rel_edge_distr_entropy': 0.9474673548740912,\n",
            " 'square_count': 0,\n",
            " 'triangle_count': 5,\n",
            " 'wedge_count': 206.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kofZ2jjlG2G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# t_adj_mats = [torch.from_numpy(m).float() for m in adj_mats]\n",
        "# t_attr_vecs = torch.eye(len(adj_mats))\n",
        "# bds = [compute_graph_statistics(adj_mats[i]) for i in range(len(t_adj_mats))]\n",
        "\n",
        "# for i in range(len(t_adj_mats)):\n",
        "#     print(t_attr_vecs[i])\n",
        "#     a = t_adj_mats[i]\n",
        "#     print('i:', i, 'bds',bds[i])\n",
        "    \n",
        "#     #show_graph(a,bds[i])\n",
        "#     show_graph(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSyCLYIjrtMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVqU0RLjyBqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_IaeEU8yBqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, av_size, d_size, gc_size, z_size, rep_size):\n",
        "        \"\"\"\n",
        "\n",
        "        :param av_size: D_A\n",
        "        :param d_size: D_X\n",
        "        :param gc_size: D'\n",
        "        :param z_size: z\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        # input parameters\n",
        "        self.z_size = z_size\n",
        "        self.attr_vec = None\n",
        "        self.gc_size = gc_size\n",
        "        self.z_size = z_size\n",
        "        self.d_size = d_size\n",
        "        self.av_size = av_size\n",
        "        self.rep_size = rep_size\n",
        "        \n",
        "        self.gc = GraphConvolution(d_size + av_size, gc_size)\n",
        "        self.gc_mu = GraphConvolution(gc_size, z_size)\n",
        "        self.gc_logvar = GraphConvolution(gc_size, z_size)\n",
        "        \n",
        "        # self.mean = nn.Sequential( nn.Linear(gc_size, z_size))\n",
        "        # self.logvar = nn.Sequential(nn.Linear(gc_size, z_size)) \n",
        "        \n",
        "        self.mean = nn.Sequential(nn.Linear(self.gc_size, int(self.gc_size/4)), \n",
        "                                  nn.BatchNorm1d(int(self.gc_size/4)), \n",
        "                                  nn.ReLU(), \n",
        "                                  nn.Linear(int(self.gc_size/4), self.z_size))\n",
        "        \n",
        "        self.logvar = nn.Sequential(nn.Linear(self.gc_size, int(self.gc_size/4)), \n",
        "                                    nn.BatchNorm1d(int(self.gc_size/4)), \n",
        "                                    nn.ReLU(), \n",
        "                                    nn.Linear(int(self.gc_size/4), self.z_size))\n",
        "    def set_attr_vec(self, attr_vec):\n",
        "        self.attr_vec = attr_vec\n",
        "\n",
        "    def forward(self, adj):\n",
        "        # print('adj size',adj.size())\n",
        "        x = get_spectral_embedding(adj, d=self.d_size)\n",
        "        # print('Encoder, before mean logvar', x.size())\n",
        "        \n",
        "        adj = normalize(adj)\n",
        "        x = cat_attr(x, self.attr_vec)\n",
        "        # print('Before gc', 'x,size', x.size(),'att size',self.attr_vec.size()  , 'adj.size', adj.size())\n",
        "        x = F.relu(self.gc(x, adj))\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        \n",
        "        # print('After gc')\n",
        "        #z_mean = self.gc_mu(x, adj)\n",
        "        #z_logvar = self.gc_logvar(x, adj)\n",
        "        \n",
        "        # create graph embedding N*D' -> 1*D'\n",
        "        # x = x.sum(0)\n",
        "        z_mean = self.mean(x)\n",
        "        z_logvar = self.logvar(x)\n",
        "        \n",
        "        # feature III here\n",
        "        # z_mean = torch.mean(z_mean, 0)\n",
        "        # z_mean = z_mean.repeat(z_logvar.shape[0], 1)\n",
        "\n",
        "        return z_mean, z_logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCC5u3soyBqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_out_size, rep_size):\n",
        "        \"\"\"\n",
        "        :param z_out_size: = z_size + len(attr_vec)\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        self.z_out_size = z_out_size\n",
        "        self.rep_size = rep_size\n",
        "        '''\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Linear(z_out_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 8),\n",
        "            nn.ReLU())\n",
        "        '''\n",
        "        self.decode = nn.Sequential(\n",
        "            #nn.Linear(z_out_size, self.rep_size),\n",
        "            #nn.BatchNorm1d(self.rep_size),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(z_out_size, int(self.rep_size)),\n",
        "            nn.BatchNorm1d(int(self.rep_size)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(self.rep_size), int(self.rep_size/4)),\n",
        "            #nn.BatchNorm1d(int(self.rep_size/2)),\n",
        "            #nn.ReLU()\n",
        "            )#nn.BatchNorm1d(int(self.rep_size/4)),\n",
        "\n",
        "        \n",
        "    def forward(self, z):\n",
        "        x = self.decode(z)\n",
        "        # x = z\n",
        "        x = torch.mm(x, x.t())\n",
        "        # x = F.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk0YzQZXyBqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, av_size, d_size, gc_size, z_size, z_out_size, rep_size):\n",
        "        \"\"\"\n",
        "        :param av_size: D_A\n",
        "        :param d_size: D_X\n",
        "        :param gc_size: D' = GCN(D_X + D_A)\n",
        "        :param z_size: original z size\n",
        "        :param z_out_size: z size + D_A (append attribute)\n",
        "        \"\"\"\n",
        "        \n",
        "        super(Generator, self).__init__()\n",
        "        self.attr_vec = None\n",
        "        self.av_size = av_size\n",
        "        self.d_zize = d_size\n",
        "        self.z_size = z_size\n",
        "        self.z_out_size = z_out_size\n",
        "        self.rep_size = rep_size\n",
        "\n",
        "        self.encoder = Encoder(av_size, d_size, gc_size, z_size, rep_size)\n",
        "        self.decoder = Decoder(z_out_size,rep_size)\n",
        "    \n",
        "    def set_attr_vec(self, attr_vec):\n",
        "        self.attr_vec = attr_vec\n",
        "        self.encoder.set_attr_vec(attr_vec)\n",
        "   \n",
        "    def forward(self, adj, training=True):\n",
        "        # print('Before encoder')\n",
        "        mean, logvar = self.encoder(adj)\n",
        "        # print('After encoder')\n",
        "        if(training):\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            reparametrized_noise = torch.randn(mean.shape, requires_grad=True).cuda()\n",
        "            reparametrized_noise = mean + std * reparametrized_noise\n",
        "        else:\n",
        "            reparametrized_noise = mean\n",
        "            # print('mean',mean)\n",
        "        # print('After variational inference')\n",
        "        x = cat_attr(reparametrized_noise, self.attr_vec)\n",
        "        # print('Before decoder')\n",
        "        rec_x = self.decoder(x)\n",
        "        return mean, logvar, rec_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47J1pjWqyBqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, av_size, d_size, gc_size, rep_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.av_size = av_size\n",
        "        self.attr_vec = None\n",
        "        self.d_size = d_size\n",
        "        self.gc_size = gc_size\n",
        "        self.rep_size = rep_size\n",
        "        self.gc = GraphConvolution(d_size + av_size, gc_size)\n",
        "        self.gc2 = GraphConvolution(gc_size, 8)\n",
        "        \n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(gc_size, int(self.rep_size/2)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(int(self.rep_size/2), 8),\n",
        "            \n",
        "            nn.LeakyReLU(0.2))\n",
        "        \n",
        "            \n",
        "\n",
        "        self.sigmoid_output = nn.Sequential(\n",
        "            nn.Linear(8, 1),\n",
        "            nn.Sigmoid())\n",
        "    \n",
        "    def set_attr_vec(self, attr_vec):\n",
        "        self.attr_vec = attr_vec\n",
        "\n",
        "    def forward(self, adj):\n",
        "        # get spectral embedding from adj, D = D_X\n",
        "        x = get_spectral_embedding(adj, d=self.d_size)\n",
        "        adj = normalize(adj)\n",
        "        x = cat_attr(x, self.attr_vec)\n",
        "\n",
        "        # GCN layer N*D -> N*D'\n",
        "        x = F.relu(self.gc(x, adj))\n",
        "        # x = F.relu(self.gc2(x, adj))\n",
        "        # x = F.dropout(x, p=0.5)\n",
        "        x = self.main(x)\n",
        "        x = x.sum(0)\n",
        "        x = self.sigmoid_output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def similarity(self, adj):\n",
        "        # get spectral embedding from adj, D = D_X\n",
        "        x = get_spectral_embedding(adj, d=self.d_size)\n",
        "        \n",
        "        # norm adj\n",
        "        adj = normalize(adj)\n",
        "\n",
        "        # concatenate attr mat, D = D_X + D_A\n",
        "        x = cat_attr(x, self.attr_vec)\n",
        "\n",
        "        # GCN layer N*D -> N*D'\n",
        "        x = F.relu(self.gc(x, adj))\n",
        "        # x = F.dropout(x, p=0.5)\n",
        "        # x = F.relu(self.gc2(x, adj))\n",
        "        # create graph embedding N*D' -> 1*D'\n",
        "        x = self.main(x)\n",
        "        x = x.mean(0)\n",
        "        # skip the last sigmoid layer\n",
        "        # x = self.main(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H4HQmyLhXRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grv2jmz_r48A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_n_indexes(arr, n):\n",
        "    idx = np.argpartition(arr, arr.size-n, axis=None)[-n:]\n",
        "    width = arr.shape[1]\n",
        "    return [divmod(i, width) for i in idx]\n",
        "\n",
        "# def topk_adj(adj, k):\n",
        "#     adj_ = adj.data.cpu().numpy()\n",
        "#     assert((adj_ == adj_.T).all())\n",
        "#     adj_ = (adj_-np.min(adj_)) / np.ptp(adj_)\n",
        "#     adj_ -= np.diag(np.diag(adj_))\n",
        "#     inds = top_n_indexes(adj_, k)\n",
        "#     res = torch.zeros(adj.shape)\n",
        "#     for ind in inds:\n",
        "#         res[ind] = 1.0\n",
        "#     return res.cuda()\n",
        "\n",
        "def topk_adj(adj, k):\n",
        "    adj_ = adj.data.cpu().numpy()\n",
        "    assert((adj_ == adj_.T).all())\n",
        "    adj_ = (adj_-np.min(adj_)) / np.ptp(adj_)\n",
        "    adj_ -= np.diag(np.diag(adj_))\n",
        "    tri_adj = np.triu(adj_)\n",
        "    inds = top_n_indexes(tri_adj, k // 2)\n",
        "    res = torch.zeros(adj.shape)\n",
        "    for ind in inds:\n",
        "        i = ind[0]\n",
        "        j = ind[1]\n",
        "        res[i, j] = 1.0\n",
        "        res[j, i] = 1.0\n",
        "    return res.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB8SbJUoSiW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_gen(model, n, attr_vec, twice_edge_num, bd =None):\n",
        "    fixed_noise = torch.randn((n, z_size), requires_grad=True).cuda()\n",
        "    if attr_vec is not None:\n",
        "        fixed_noise = cat_attr(fixed_noise, attr_vec.cuda())\n",
        "    a_ = model.decoder(fixed_noise)\n",
        "    #print(F.sigmoid(a_))\n",
        "    a_ = topk_adj(F.sigmoid(a_), twice_edge_num)\n",
        "    #print(a_)\n",
        "    if bd:\n",
        "        show_graph(a_, bd)\n",
        "    else:\n",
        "        show_graph(a_)\n",
        "def gen_adj(model, n, e, attr_vec):\n",
        "    fixed_noise = torch.randn((n, z_size), requires_grad=True).cuda()\n",
        "    fixed_noise = cat_attr(fixed_noise, attr_vec)\n",
        "    rec_adj = model.decoder(fixed_noise)\n",
        "    return topk_adj(rec_adj, e*2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OdFeFLaIw5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def eval(adj, base_adj=None):\n",
        "    if not isinstance(adj, np.ndarray):\n",
        "        adj_ = adj.data.cpu().numpy()\n",
        "    else:\n",
        "        adj_ = copy.deepcopy(adj)\n",
        "    \n",
        "    adj_ -= np.diag(np.diag(adj_))\n",
        "    gr = nx.from_numpy_array(adj_)\n",
        "    assert((adj_ == adj_.T).all())\n",
        "\n",
        "    d = compute_graph_statistics(adj_)\n",
        "    # pprint(d)\n",
        "    \n",
        "    if base_adj is not None:\n",
        "        # base_adj = base_adj.numpy()\n",
        "        base_gr = nx.from_numpy_array(base_adj)\n",
        "        bd = compute_graph_statistics(base_adj)\n",
        "        diff_d = {}\n",
        "        \n",
        "        for k in list(d.keys()):\n",
        "            diff_d[k] = round(abs(d[k] - bd[k]), 4)\n",
        "    return diff_d\n",
        "\n",
        "\n",
        "def gen_adj(model, n, e, attr_vec):\n",
        "    fixed_noise = torch.randn((n, z_size), requires_grad=True).cuda()\n",
        "    fixed_noise = cat_attr(fixed_noise, attr_vec)\n",
        "    rec_adj = model.decoder(fixed_noise)\n",
        "    final_adj = topk_adj(rec_adj, e*2)\n",
        "    return final_adj\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0793f57a-bea2-4f61-dc67-d6aedda23d53",
        "id": "-mpqNO2MFGgl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "source": [
        "import random\n",
        "av_size = 10  # set 0 if you do not need attr_vec\n",
        "z_size = 6\n",
        "rep_size = 32\n",
        "d_size = 2\n",
        "gc_size = 16\n",
        "\n",
        "z_out_size = z_size + av_size\n",
        "\n",
        "adj_thresh = .6\n",
        "max_epochs = 30\n",
        "# lr = 3e-4\n",
        "lr = 0.003\n",
        "\n",
        "beta = 5\n",
        "beta2 = 0.1\n",
        "alpha = 0.1\n",
        "gamma = 15\n",
        "\n",
        "G = Generator(\n",
        "    av_size=av_size,\n",
        "    d_size=d_size,\n",
        "    gc_size=gc_size,\n",
        "    z_size=z_size,\n",
        "    z_out_size=z_out_size,\n",
        "    rep_size=rep_size\n",
        ").cuda()\n",
        "\n",
        "D = Discriminator(\n",
        "    av_size=av_size,\n",
        "    d_size=d_size,\n",
        "    gc_size=gc_size,\n",
        "    rep_size = rep_size\n",
        ").cuda()\n",
        "\n",
        "criterion_bce = nn.BCELoss()\n",
        "criterion_bce.cuda()\n",
        "\n",
        "# This three are for A A' loss\n",
        "loss_MSE = nn.MSELoss()\n",
        "loss_MSE.cuda()  \n",
        "\n",
        "loss_BCE_logits = nn.BCEWithLogitsLoss()#size_average=False)\n",
        "loss_BCE_logits.cuda()      \n",
        "\n",
        "loss_BCE = nn.BCELoss()#size_average=False)\n",
        "loss_BCE.cuda()     \n",
        "\n",
        "\n",
        "# opt_enc = optim.RMSprop(G.encoder.parameters(), lr=lr)\n",
        "# opt_dec = optim.RMSprop(G.decoder.parameters(), lr=lr)\n",
        "# opt_dis = optim.RMSprop(D.parameters(), lr=lr * alpha)\n",
        "\n",
        "\n",
        "opt_enc = optim.Adam(G.encoder.parameters(), lr=lr)\n",
        "opt_dec = optim.Adam(G.decoder.parameters(), lr=lr)\n",
        "opt_dis = optim.Adam(D.parameters(), lr=lr * alpha)\n",
        "\n",
        "\n",
        "# training_index = list(range(0, av_size))\n",
        "training_index = list(range(0, len(train_adj_mats)))\n",
        "\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    ct = 0\n",
        "    training_list = []\n",
        "    D_real_list, D_rec_enc_list, D_rec_noise_list, D_list, Encoder_list = [], [], [], [], []\n",
        "    # g_loss_list, rec_loss_list, prior_loss_list = [], [], []\n",
        "    g_loss_list, rec_loss_list, prior_loss_list, aa_loss_list = [], [], [], []\n",
        "    random.shuffle(training_index)\n",
        "    for i in training_index:\n",
        "\n",
        "        ones_label = Variable(torch.ones(1)).cuda()\n",
        "        zeros_label = Variable(torch.zeros(1)).cuda()\n",
        "        # adj = Variable(train_adj_mats[i]).cuda()\n",
        "        adj = Variable(torch.from_numpy(train_adj_mats[i]).float()).cuda()\n",
        "        \n",
        "        #if adj.shape[0] <= d_size + 2 :\n",
        "        #    continue\n",
        "        if adj.shape[0] < d_size+2 or adj.shape[0]>50:\n",
        "            continue   \n",
        "        if av_size == 0:\n",
        "            attr_vec = None\n",
        "        else:\n",
        "            #attr_vec = Variable(train_attr_vecs[i, :]).cuda()\n",
        "            attr_vec = Variable(torch.from_numpy(train_attr_vecs[i]).float()).cuda()\n",
        "        \n",
        "        ct += 1\n",
        "        training_list.append(i)\n",
        "        \n",
        "        # edge_num = train_adj_mats[i].sum()\n",
        "        G.set_attr_vec(attr_vec)\n",
        "        D.set_attr_vec(attr_vec)\n",
        "        \n",
        "        norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
        "        pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
        "        # print('pos_weight', pos_weight)\n",
        "        \n",
        "        mean, logvar, rec_adj = G(adj)\n",
        "        \n",
        "        noisev = torch.randn(mean.shape, requires_grad=True).cuda()\n",
        "        noisev = cat_attr(noisev, attr_vec)\n",
        "        rec_noise = G.decoder(noisev)\n",
        "        \n",
        "        \n",
        "        e = int(np.sum(train_adj_mats[i])) // 2\n",
        "        \n",
        "        c_adj = topk_adj(F.sigmoid(rec_adj), e*2)\n",
        "        c_noise = topk_adj(F.sigmoid(rec_noise), e*2)\n",
        "\n",
        "        \n",
        "        # train discriminator\n",
        "        output = D(adj)\n",
        "        errD_real = criterion_bce(output, ones_label)\n",
        "        D_real_list.append(output.data.mean())\n",
        "        #output = D(rec_adj)\n",
        "        output = D(c_adj)\n",
        "        errD_rec_enc = criterion_bce(output, zeros_label)\n",
        "        D_rec_enc_list.append(output.data.mean())\n",
        "        # output = D(rec_noise)\n",
        "        output = D(c_noise)\n",
        "       \n",
        "        errD_rec_noise = criterion_bce(output, zeros_label)\n",
        "        D_rec_noise_list.append(output.data.mean())\n",
        "        \n",
        "        dis_img_loss = errD_real + errD_rec_enc + errD_rec_noise\n",
        "        # print (\"print (dis_img_loss)\", dis_img_loss)\n",
        "        D_list.append(dis_img_loss.data.mean())\n",
        "        opt_dis.zero_grad()\n",
        "        dis_img_loss.backward(retain_graph=True)\n",
        "        opt_dis.step()\n",
        "        \n",
        "        \n",
        "        # AA_loss b/w rec_adj and adj\n",
        "        # aa_loss = loss_MSE(rec_adj, adj)\n",
        "        \n",
        "        loss_BCE_logits = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        loss_BCE_logits.cuda()\n",
        "        \n",
        "        aa_loss = loss_BCE_logits(rec_adj, adj)\n",
        "       \n",
        "        #print(c_adj,c_adj)\n",
        "        #aa_loss = loss_BCE(c_adj, adj)\n",
        "        \n",
        "        # train decoder\n",
        "        output = D(adj)\n",
        "        errD_real = criterion_bce(output, ones_label)\n",
        "        # output = D(rec_adj)\n",
        "        output = D(c_adj)\n",
        "            \n",
        "        errD_rec_enc = criterion_bce(output, zeros_label)\n",
        "        errG_rec_enc = criterion_bce(output, ones_label)\n",
        "        # output = D(rec_noise)\n",
        "        output = D(c_noise)\n",
        "        \n",
        "        errD_rec_noise = criterion_bce(output, zeros_label)\n",
        "        errG_rec_noise = criterion_bce(output, ones_label)\n",
        "        \n",
        "        similarity_rec_enc = D.similarity(c_adj)\n",
        "        similarity_data = D.similarity(adj)\n",
        "        \n",
        "        dis_img_loss = errD_real + errD_rec_enc + errD_rec_noise \n",
        "        # print (dis_img_loss)\n",
        "        \n",
        "        \n",
        "        # gen_img_loss = norm*(aa_loss + errG_rec_enc  + errG_rec_noise)- dis_img_loss #- dis_img_loss #aa_loss #+ errG_rec_enc  + errG_rec_noise # - dis_img_loss \n",
        "        gen_img_loss = - dis_img_loss #norm*(aa_loss) #\n",
        "        \n",
        "        g_loss_list.append(gen_img_loss.data.mean())\n",
        "        rec_loss = ((similarity_rec_enc - similarity_data) ** 2).mean()\n",
        "        rec_loss_list.append(rec_loss.data.mean())\n",
        "        # err_dec =  gamma * rec_loss + gen_img_loss\n",
        "        \n",
        "        err_dec =  gamma * rec_loss + gen_img_loss\n",
        "        opt_dec.zero_grad()\n",
        "        err_dec.backward(retain_graph=True)\n",
        "        opt_dec.step()\n",
        "        \n",
        "        # train encoder\n",
        "        # fix me: sum version of prior loss\n",
        "        pl = []\n",
        "        for j in range(mean.size()[0]):\n",
        "            prior_loss = 1 + logvar[j, :] - mean[j, :].pow(2) - logvar[j, :].exp()\n",
        "            prior_loss = (-0.5 * torch.sum(prior_loss))/torch.numel(mean[j, :].data)\n",
        "            pl.append(prior_loss)\n",
        "        prior_loss_list.append(sum(pl))\n",
        "        err_enc = sum(pl) + gen_img_loss + beta * (rec_loss ) #+ beta2* norm* aa_loss\n",
        "        opt_enc.zero_grad()\n",
        "        err_enc.backward()\n",
        "        opt_enc.step()\n",
        "        Encoder_list.append(err_enc.data.mean())\n",
        "    print('%d data training in total '%ct) \n",
        "    print('[%d/%d]: D_real:%.4f, D_enc:%.4f, D_noise:%.4f, Loss_D:%.4f, Loss_G:%.4f, rec_loss:%.4f, prior_loss:%.4f' \n",
        "                   % (epoch, \n",
        "                      max_epochs, \n",
        "                      torch.mean(torch.stack(D_real_list)), \n",
        "                      torch.mean(torch.stack(D_rec_enc_list)), \n",
        "                      torch.mean(torch.stack(D_rec_noise_list)), \n",
        "                      torch.mean(torch.stack(D_list)), \n",
        "                      torch.mean(torch.stack(g_loss_list)),\n",
        "                      torch.mean(torch.stack(rec_loss_list)),\n",
        "                      torch.mean(torch.stack(prior_loss_list))))\n",
        "    '''\n",
        "    print('Training set')\n",
        "    for i in range(3):\n",
        "        base_adj = train_adj_mats[i]\n",
        "        \n",
        "        if base_adj.shape[0] <= d_size:\n",
        "            continue\n",
        "        print('Base Adj_size: ',base_adj.shape)\n",
        "        attr_vec = Variable(torch.from_numpy(train_attr_vecs[i]).float()).cuda()\n",
        "\n",
        "        # add a new line\n",
        "        G.set_attr_vec(attr_vec)\n",
        "\n",
        "        print('Show sample')\n",
        "        sample_adj = gen_adj(G, base_adj.shape[0], int(np.sum(base_adj)) // 2, attr_vec)\n",
        "        show_graph(sample_adj, base_adj=base_adj, remove_isolated=True)\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26 data training in total \n",
            "[0/30]: D_real:0.4134, D_enc:0.4057, D_noise:0.4074, Loss_D:1.9921, Loss_G:-1.9908, rec_loss:0.0001, prior_loss:1.4848\n",
            "26 data training in total \n",
            "[1/30]: D_real:0.3888, D_enc:0.3797, D_noise:0.3809, Loss_D:1.9528, Loss_G:-1.9477, rec_loss:0.0001, prior_loss:0.6663\n",
            "26 data training in total \n",
            "[2/30]: D_real:0.3756, D_enc:0.3661, D_noise:0.3656, Loss_D:1.9384, Loss_G:-1.9310, rec_loss:0.0001, prior_loss:0.3434\n",
            "26 data training in total \n",
            "[3/30]: D_real:0.3796, D_enc:0.3661, D_noise:0.3682, Loss_D:1.9197, Loss_G:-1.9178, rec_loss:0.0001, prior_loss:0.2122\n",
            "26 data training in total \n",
            "[4/30]: D_real:0.3847, D_enc:0.3712, D_noise:0.3704, Loss_D:1.9105, Loss_G:-1.9116, rec_loss:0.0001, prior_loss:0.1469\n",
            "26 data training in total \n",
            "[5/30]: D_real:0.3789, D_enc:0.3631, D_noise:0.3651, Loss_D:1.9014, Loss_G:-1.9028, rec_loss:0.0001, prior_loss:0.0854\n",
            "26 data training in total \n",
            "[6/30]: D_real:0.3836, D_enc:0.3678, D_noise:0.3653, Loss_D:1.8918, Loss_G:-1.8938, rec_loss:0.0002, prior_loss:0.0589\n",
            "26 data training in total \n",
            "[7/30]: D_real:0.3758, D_enc:0.3590, D_noise:0.3572, Loss_D:1.8854, Loss_G:-1.8815, rec_loss:0.0001, prior_loss:0.0397\n",
            "26 data training in total \n",
            "[8/30]: D_real:0.3776, D_enc:0.3605, D_noise:0.3574, Loss_D:1.8818, Loss_G:-1.8740, rec_loss:0.0001, prior_loss:0.0266\n",
            "26 data training in total \n",
            "[9/30]: D_real:0.3777, D_enc:0.3524, D_noise:0.3561, Loss_D:1.8659, Loss_G:-1.8728, rec_loss:0.0001, prior_loss:0.0192\n",
            "26 data training in total \n",
            "[10/30]: D_real:0.3775, D_enc:0.3538, D_noise:0.3535, Loss_D:1.8639, Loss_G:-1.8608, rec_loss:0.0001, prior_loss:0.0150\n",
            "26 data training in total \n",
            "[11/30]: D_real:0.3862, D_enc:0.3596, D_noise:0.3622, Loss_D:1.8622, Loss_G:-1.8538, rec_loss:0.0003, prior_loss:0.0109\n",
            "26 data training in total \n",
            "[12/30]: D_real:0.3827, D_enc:0.3544, D_noise:0.3530, Loss_D:1.8485, Loss_G:-1.8524, rec_loss:0.0002, prior_loss:0.0081\n",
            "26 data training in total \n",
            "[13/30]: D_real:0.3829, D_enc:0.3503, D_noise:0.3455, Loss_D:1.8299, Loss_G:-1.8386, rec_loss:0.0002, prior_loss:0.0080\n",
            "26 data training in total \n",
            "[14/30]: D_real:0.3816, D_enc:0.3464, D_noise:0.3478, Loss_D:1.8329, Loss_G:-1.8333, rec_loss:0.0003, prior_loss:0.0098\n",
            "26 data training in total \n",
            "[15/30]: D_real:0.3831, D_enc:0.3397, D_noise:0.3360, Loss_D:1.8004, Loss_G:-1.8351, rec_loss:0.0003, prior_loss:0.0090\n",
            "26 data training in total \n",
            "[16/30]: D_real:0.3784, D_enc:0.3426, D_noise:0.3352, Loss_D:1.8164, Loss_G:-1.8254, rec_loss:0.0003, prior_loss:0.0061\n",
            "26 data training in total \n",
            "[17/30]: D_real:0.3807, D_enc:0.3351, D_noise:0.3354, Loss_D:1.7975, Loss_G:-1.7930, rec_loss:0.0004, prior_loss:0.0050\n",
            "26 data training in total \n",
            "[18/30]: D_real:0.3898, D_enc:0.3418, D_noise:0.3402, Loss_D:1.7931, Loss_G:-1.8341, rec_loss:0.0004, prior_loss:0.0041\n",
            "26 data training in total \n",
            "[19/30]: D_real:0.3845, D_enc:0.3277, D_noise:0.3335, Loss_D:1.7785, Loss_G:-1.7762, rec_loss:0.0004, prior_loss:0.0034\n",
            "26 data training in total \n",
            "[20/30]: D_real:0.3879, D_enc:0.3305, D_noise:0.3278, Loss_D:1.7665, Loss_G:-1.7610, rec_loss:0.0006, prior_loss:0.0029\n",
            "26 data training in total \n",
            "[21/30]: D_real:0.3877, D_enc:0.3256, D_noise:0.3256, Loss_D:1.7600, Loss_G:-1.7349, rec_loss:0.0006, prior_loss:0.0023\n",
            "26 data training in total \n",
            "[22/30]: D_real:0.3950, D_enc:0.3204, D_noise:0.3339, Loss_D:1.7454, Loss_G:-1.7607, rec_loss:0.0007, prior_loss:0.0024\n",
            "26 data training in total \n",
            "[23/30]: D_real:0.3966, D_enc:0.3217, D_noise:0.3183, Loss_D:1.7209, Loss_G:-1.7348, rec_loss:0.0008, prior_loss:0.0024\n",
            "26 data training in total \n",
            "[24/30]: D_real:0.4051, D_enc:0.3256, D_noise:0.3177, Loss_D:1.7130, Loss_G:-1.7027, rec_loss:0.0006, prior_loss:0.0013\n",
            "26 data training in total \n",
            "[25/30]: D_real:0.4054, D_enc:0.3198, D_noise:0.3181, Loss_D:1.7040, Loss_G:-1.6912, rec_loss:0.0011, prior_loss:0.0025\n",
            "26 data training in total \n",
            "[26/30]: D_real:0.4137, D_enc:0.3168, D_noise:0.3181, Loss_D:1.6787, Loss_G:-1.6740, rec_loss:0.0012, prior_loss:0.0013\n",
            "26 data training in total \n",
            "[27/30]: D_real:0.4140, D_enc:0.3082, D_noise:0.3083, Loss_D:1.6590, Loss_G:-1.6627, rec_loss:0.0010, prior_loss:0.0010\n",
            "26 data training in total \n",
            "[28/30]: D_real:0.4202, D_enc:0.3085, D_noise:0.3166, Loss_D:1.6591, Loss_G:-1.6442, rec_loss:0.0010, prior_loss:0.0010\n",
            "26 data training in total \n",
            "[29/30]: D_real:0.4250, D_enc:0.3142, D_noise:0.3023, Loss_D:1.6350, Loss_G:-1.6174, rec_loss:0.0012, prior_loss:0.0006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg96bEQsEhhK",
        "colab_type": "code",
        "outputId": "971e8f5f-8e29-4245-d290-4499fce48d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sorted(training_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 3, 9, 10, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 27, 31, 33, 34, 39, 40, 42, 43, 45, 48, 49, 51]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJNh8qxGokgU",
        "colab_type": "code",
        "outputId": "3f6abbb0-6316-4d57-f266-824f9c087640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1448
        }
      },
      "source": [
        "#### EVAL\n",
        "# Evaluation\n",
        "\n",
        "diffs ={}\n",
        "for i in training_list:\n",
        "    print('Graph %d'%i)\n",
        "    base_adj = train_adj_mats[i] #Variable(torch.from_numpy(train_adj_mats[i]).float()).cuda()\n",
        "        \n",
        "    if base_adj.shape[0] < d_size + 2 or base_adj.shape[0] >50:\n",
        "            continue\n",
        "            \n",
        "    # attr_vec = Variable(train_attr_vecs[i].float()).cuda()\n",
        "    attr_vec = Variable(torch.from_numpy(train_attr_vecs[i]).float()).cuda()\n",
        "    \n",
        "    # add a new line\n",
        "    G.set_attr_vec(attr_vec)\n",
        "    for u in range(3):\n",
        "        sample_adj = gen_adj(G, base_adj.shape[0], int((base_adj).sum()) // 2, attr_vec)\n",
        "        # show_graph(sample_adj, base_adj)\n",
        "        diff_d = eval(sample_adj, base_adj=base_adj)\n",
        "        \n",
        "        for k in list(diff_d.keys()):\n",
        "            if k not in diffs.keys():\n",
        "                diffs[k] = []\n",
        "            diffs[k].append(diff_d[k])\n",
        "            \n",
        "for k in list(diffs.keys()):  \n",
        "    diffs[k] = np.mean(diffs[k])    \n",
        "print(diffs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph 21\n",
            "Graph 42\n",
            "Graph 48\n",
            "Graph 31\n",
            "Graph 12\n",
            "Graph 24\n",
            "Graph 45\n",
            "Graph 33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Graph 9\n",
            "Graph 22\n",
            "Graph 16\n",
            "Graph 51\n",
            "Graph 49\n",
            "Graph 1\n",
            "Graph 23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Graph 14\n",
            "Graph 10\n",
            "Graph 40\n",
            "Graph 27\n",
            "Graph 3\n",
            "Graph 39\n",
            "Graph 25\n",
            "Graph 13\n",
            "Graph 43\n",
            "Graph 15\n",
            "Graph 34\n",
            "{'d_max': 1.4743589743589745, 'd_min': 0.717948717948718, 'd': 0.0, 'node_num': 0.0, 'edge_num': 0.0, 'LCC': 5.884615384615385, 'wedge_count': 33.11538461538461, 'claw_count': 99.91025641025641, 'triangle_count': 11.128205128205128, 'square_count': 8.051282051282051, 'power_law_exp': 0.6434230769230769, 'gini': 0.15637564102564105, 'rel_edge_distr_entropy': 0.10284230769230768, 'assortativity': nan, 'clustering_coefficient': 0.58505, 'n_components': 5.358974358974359, 'cpl': 1.0202205128205126}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MAG8Wd6r_a2",
        "colab_type": "code",
        "outputId": "f34fcd3b-3af7-4a1e-e18e-ce2823bab682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "pprint(diffs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LCC': 5.884615384615385,\n",
            " 'assortativity': nan,\n",
            " 'claw_count': 99.91025641025641,\n",
            " 'clustering_coefficient': 0.58505,\n",
            " 'cpl': 1.0202205128205126,\n",
            " 'd': 0.0,\n",
            " 'd_max': 1.4743589743589745,\n",
            " 'd_min': 0.717948717948718,\n",
            " 'edge_num': 0.0,\n",
            " 'gini': 0.15637564102564105,\n",
            " 'n_components': 5.358974358974359,\n",
            " 'node_num': 0.0,\n",
            " 'power_law_exp': 0.6434230769230769,\n",
            " 'rel_edge_distr_entropy': 0.10284230769230768,\n",
            " 'square_count': 8.051282051282051,\n",
            " 'triangle_count': 11.128205128205128,\n",
            " 'wedge_count': 33.11538461538461}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENjZfjpJDRho",
        "colab_type": "code",
        "outputId": "34def3a1-3e62-422d-a310-df4ed95fe37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "#### EVAL\n",
        "# UNSEEN\n",
        "diffs ={}\n",
        "for i in range(len(test_adj_mats)):\n",
        "    \n",
        "    base_adj = test_adj_mats[i]   \n",
        "    if base_adj.shape[0] < d_size + 2 or base_adj.shape[0] >50:\n",
        "            continue\n",
        "    attr_vec = Variable(torch.from_numpy(test_attr_vecs[i]).float()).cuda()\n",
        "    print('Graph %d'%i, base_adj.shape[0] )\n",
        "    \n",
        "    G.set_attr_vec(attr_vec)\n",
        "    for u in range(3):\n",
        "        sample_adj = gen_adj(G, base_adj.shape[0], int((base_adj).sum()) // 2, attr_vec)\n",
        "        # show_graph(sample_adj, base_adj)\n",
        "        diff_d = eval(sample_adj, base_adj=base_adj)\n",
        "        # pprint( diff_d)\n",
        "        for k in list(diff_d.keys()):\n",
        "            if k not in diffs.keys():\n",
        "                diffs[k] = []\n",
        "            diffs[k].append(diff_d[k])\n",
        "            \n",
        "for k in list(diffs.keys()):  \n",
        "    diffs[k] = np.mean(diffs[k])    \n",
        "print(diffs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph 0 30\n",
            "Graph 5 12\n",
            "Graph 6 15\n",
            "Graph 8 9\n",
            "{'d_max': 5.75, 'd_min': 0.9166666666666666, 'd': 0.0, 'node_num': 0.0, 'edge_num': 0.0, 'LCC': 4.5, 'wedge_count': 62.0, 'claw_count': 343.75, 'triangle_count': 8.0, 'square_count': 2.9166666666666665, 'power_law_exp': 0.8341666666666665, 'gini': 0.22536666666666663, 'rel_edge_distr_entropy': 0.14086666666666667, 'assortativity': 0.2662916666666667, 'clustering_coefficient': 0.287, 'n_components': 4.416666666666667, 'cpl': 1.2821500000000001}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H_dc5udDRnB",
        "colab_type": "code",
        "outputId": "3432d755-6d24-4863-f217-17ef0b162b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "pprint(diffs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LCC': 4.5,\n",
            " 'assortativity': 0.2662916666666667,\n",
            " 'claw_count': 343.75,\n",
            " 'clustering_coefficient': 0.287,\n",
            " 'cpl': 1.2821500000000001,\n",
            " 'd': 0.0,\n",
            " 'd_max': 5.75,\n",
            " 'd_min': 0.9166666666666666,\n",
            " 'edge_num': 0.0,\n",
            " 'gini': 0.22536666666666663,\n",
            " 'n_components': 4.416666666666667,\n",
            " 'node_num': 0.0,\n",
            " 'power_law_exp': 0.8341666666666665,\n",
            " 'rel_edge_distr_entropy': 0.14086666666666667,\n",
            " 'square_count': 2.9166666666666665,\n",
            " 'triangle_count': 8.0,\n",
            " 'wedge_count': 62.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHWjJprXDRsd",
        "colab_type": "code",
        "outputId": "840f986a-2b8d-478f-ea94-43f64532fa4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "##BASELINE\n",
        "import random\n",
        "av_size = 10  # set 0 if you do not need attr_vec\n",
        "z_size = 6\n",
        "d_size = 2\n",
        "gc_size = 16\n",
        "rep_size = 16\n",
        "z_out_size = z_size + av_size\n",
        "\n",
        "adj_thresh = .5\n",
        "max_epochs = 10\n",
        "lr = 0.005\n",
        "\n",
        "\n",
        "beta = 5\n",
        "alpha = 0.1\n",
        "gamma = 15\n",
        "\n",
        "G = Generator(\n",
        "    av_size=av_size,\n",
        "    d_size=d_size,\n",
        "    gc_size=gc_size,\n",
        "    z_size=z_size,\n",
        "    z_out_size=z_out_size,\n",
        "    rep_size=rep_size\n",
        ").cuda()\n",
        "\n",
        "criterion_bce = nn.BCELoss()\n",
        "criterion_bce.cuda()\n",
        "\n",
        "loss_MSE = nn.MSELoss()\n",
        "loss_MSE.cuda()   \n",
        "loss_BCE = nn.BCELoss()\n",
        "loss_BCE.cuda()      \n",
        "  \n",
        "opt_vae = optim.Adam(G.parameters(), lr=lr)\n",
        "\n",
        "training_index = list(range(0, len(train_adj_mats)))\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    D_real_list, D_rec_enc_list, D_rec_noise_list, D_list = [], [], [], []\n",
        "    # g_loss_list, rec_loss_list, prior_loss_list = [], [], []\n",
        "    g_loss_list, rec_loss_list, prior_loss_list, aa_loss_list = [], [], [], []\n",
        "    # Only train the 8th graph\n",
        "    # for i in range(8, 9): \n",
        "    ct = 0\n",
        "    \n",
        "    training_list = []\n",
        "    random.shuffle(training_index)\n",
        "    for i in training_index:\n",
        "        ones_label = Variable(torch.ones(1)).cuda()\n",
        "        zeros_label = Variable(torch.zeros(1)).cuda()\n",
        "        edge_num = np.sum(train_adj_mats[i])\n",
        "        \n",
        "        e = int(np.sum(train_adj_mats[i]) // 2)\n",
        "        # adj = Variable(train_adj_mats[i]).cuda()\n",
        "        adj = Variable(torch.from_numpy(train_adj_mats[i]).float()).cuda()\n",
        "        \n",
        "        # if adj.shape[0] <= d_size+2\n",
        "        if adj.shape[0] < d_size+2 or adj.shape[0]>50:\n",
        "            continue\n",
        "        ct += 1\n",
        "        training_list.append(i)\n",
        "        if av_size == 0:\n",
        "            attr_vec = None\n",
        "        else:\n",
        "            #attr_vec = Variable(train_attr_vecs[i]).cuda()\n",
        "            attr_vec = Variable(torch.from_numpy(train_attr_vecs[i]).float()).cuda()\n",
        "        G.set_attr_vec(attr_vec)\n",
        "\n",
        "        \n",
        "        norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
        "        pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
        "        # print('pos_weight', pos_weight)\n",
        "        mean, logvar, rec_adj = G(adj, training=True)\n",
        "        \n",
        "        noisev = torch.randn(mean.shape, requires_grad=True).cuda()\n",
        "        noisev = cat_attr(noisev, attr_vec)\n",
        "        rec_noise = G.decoder(noisev)\n",
        "       \n",
        "\n",
        "        c_adj = topk_adj(F.sigmoid(rec_adj), e*2)\n",
        "        aa_loss = loss_BCE(c_adj, adj)\n",
        "        \n",
        "        loss_BCE_logits = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        loss_BCE_logits.cuda()\n",
        "        aa_loss = loss_BCE_logits(rec_adj, adj)\n",
        "     \n",
        "        pl = []\n",
        "        for j in range(mean.size()[0]):\n",
        "            prior_loss = 1 + logvar[j, :] - mean[j, :].pow(2) - logvar[j, :].exp()\n",
        "            prior_loss = (-0.5 * torch.sum(prior_loss))/torch.numel(mean[j, :].data)\n",
        "            pl.append(prior_loss)\n",
        "        prior_loss_list.append(sum(pl))\n",
        "        kl_loss = sum(pl) # /torch.numel(mean[i, :].data)\n",
        "        vae_loss = kl_loss + norm*aa_loss\n",
        "        opt_vae.zero_grad()\n",
        "        vae_loss.backward() #retain_graph=True\n",
        "        opt_vae.step()\n",
        "    print('%d data training in total '%ct)\n",
        "    print('[%d/%d]: vae_loss:%.4f, prior_loss:%.4f' \n",
        "                   % (epoch, \n",
        "                      max_epochs, \n",
        "                      torch.mean(torch.stack([vae_loss])),\n",
        "                      torch.mean(torch.stack(prior_loss_list))))\n",
        "    '''\n",
        "    print('Training set')\n",
        "    for i in range(10):  \n",
        "        base_adj = train_adj_mats[i]\n",
        "        \n",
        "        if base_adj.shape[0] <= d_size + 2 or base_adj.shape[0] >50:\n",
        "            continue\n",
        "        print('Base Adj_size: ',base_adj.shape)\n",
        "        attr_vec = Variable(torch.from_numpy(train_attr_vecs[i]).float()).cuda()\n",
        "        # add a new line\n",
        "        G.set_attr_vec(attr_vec)\n",
        "        print('Show sample')\n",
        "        sample_adj = gen_adj(G, base_adj.shape[0], int(np.sum(base_adj)) // 2, attr_vec)\n",
        "        show_graph(sample_adj, base_adj=base_adj, remove_isolated=True)\n",
        "\n",
        "    # test testing set\n",
        "    print('Testing set')\n",
        "    for i in range(10):\n",
        "        base_adj = test_adj_mats[i]\n",
        "        if base_adj.shape[0] <= d_size + 2 :# or base_adj.shape[0] >50:\n",
        "            continue\n",
        "        attr_vec = Variable(torch.from_numpy(test_attr_vecs[i]).float()).cuda()\n",
        "        rec_adj = gen_adj(G, base_adj.shape[0], int(np.sum(base_adj)) // 2, attr_vec)\n",
        "        show_graph(rec_adj, base_adj=base_adj, remove_isolated=True)\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26 data training in total \n",
            "[0/10]: vae_loss:1.5728, prior_loss:1.1356\n",
            "26 data training in total \n",
            "[1/10]: vae_loss:0.7922, prior_loss:0.4145\n",
            "26 data training in total \n",
            "[2/10]: vae_loss:0.7446, prior_loss:0.1819\n",
            "26 data training in total \n",
            "[3/10]: vae_loss:0.7223, prior_loss:0.0966\n",
            "26 data training in total \n",
            "[4/10]: vae_loss:0.7043, prior_loss:0.0452\n",
            "26 data training in total \n",
            "[5/10]: vae_loss:0.7258, prior_loss:0.0224\n",
            "26 data training in total \n",
            "[6/10]: vae_loss:0.6963, prior_loss:0.0135\n",
            "26 data training in total \n",
            "[7/10]: vae_loss:0.7006, prior_loss:0.0074\n",
            "26 data training in total \n",
            "[8/10]: vae_loss:0.7015, prior_loss:0.0064\n",
            "26 data training in total \n",
            "[9/10]: vae_loss:0.6955, prior_loss:0.0054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx1RXaXYDRyO",
        "colab_type": "code",
        "outputId": "3d435c80-f83e-46bb-928c-2087a30a84f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#### EVAL\n",
        "# UNSEEN\n",
        "diffs ={}\n",
        "for i in range(len(test_adj_mats)):\n",
        "    \n",
        "    base_adj = test_adj_mats[i]   \n",
        "    if base_adj.shape[0] < d_size + 2 or base_adj.shape[0] >50:\n",
        "            continue\n",
        "    attr_vec = Variable(torch.from_numpy(test_attr_vecs[i]).float()).cuda()\n",
        "    print('Graph %d'%i, base_adj.shape[0] )\n",
        "    \n",
        "    G.set_attr_vec(attr_vec)\n",
        "    for u in range(3):\n",
        "        sample_adj = gen_adj(G, base_adj.shape[0], int((base_adj).sum()) // 2, attr_vec)\n",
        "        # show_graph(sample_adj, base_adj)\n",
        "        diff_d = eval(sample_adj, base_adj=base_adj)\n",
        "        \n",
        "        for k in list(diff_d.keys()):\n",
        "            if k not in diffs.keys():\n",
        "                diffs[k] = []\n",
        "            diffs[k].append(diff_d[k])\n",
        "            \n",
        "for k in list(diffs.keys()):  \n",
        "    diffs[k] = np.mean(diffs[k])    \n",
        "print(diffs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph 0 30\n",
            "Graph 5 12\n",
            "Graph 6 15\n",
            "Graph 8 9\n",
            "{'d_max': 3.8333333333333335, 'd_min': 1.0, 'd': 0.0, 'node_num': 0.0, 'edge_num': 0.0, 'LCC': 5.666666666666667, 'wedge_count': 49.583333333333336, 'claw_count': 170.75, 'triangle_count': 10.583333333333334, 'square_count': 5.0, 'power_law_exp': 0.8287083333333333, 'gini': 0.24125833333333332, 'rel_edge_distr_entropy': 0.14615833333333336, 'assortativity': 0.25710000000000005, 'clustering_coefficient': 0.24575833333333333, 'n_components': 5.666666666666667, 'cpl': 1.2760583333333335}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khIY51G8DR1N",
        "colab_type": "code",
        "outputId": "af3aae2a-1ce3-4ee6-80a4-58b81e97ef92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "pprint(diffs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LCC': 5.666666666666667,\n",
            " 'assortativity': 0.25710000000000005,\n",
            " 'claw_count': 170.75,\n",
            " 'clustering_coefficient': 0.24575833333333333,\n",
            " 'cpl': 1.2760583333333335,\n",
            " 'd': 0.0,\n",
            " 'd_max': 3.8333333333333335,\n",
            " 'd_min': 1.0,\n",
            " 'edge_num': 0.0,\n",
            " 'gini': 0.24125833333333332,\n",
            " 'n_components': 5.666666666666667,\n",
            " 'node_num': 0.0,\n",
            " 'power_law_exp': 0.8287083333333333,\n",
            " 'rel_edge_distr_entropy': 0.14615833333333336,\n",
            " 'square_count': 5.0,\n",
            " 'triangle_count': 10.583333333333334,\n",
            " 'wedge_count': 49.583333333333336}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVcncmyfDRwL",
        "colab_type": "code",
        "outputId": "6bb6f141-4752-4962-c468-cc7604d6b9bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1686
        }
      },
      "source": [
        "##SEEN\n",
        "#### EVAL\n",
        "# Evaluation\n",
        "\n",
        "diffs ={}\n",
        "for i in training_list:\n",
        "    \n",
        "    base_adj = train_adj_mats[i]   \n",
        "    if base_adj.shape[0] < d_size + 2 or base_adj.shape[0] >50:\n",
        "            continue\n",
        "    attr_vec = Variable(torch.from_numpy(train_attr_vecs[i]).float()).cuda()\n",
        "    print('Graph %d'%i, base_adj.shape[0] )\n",
        "    \n",
        "    G.set_attr_vec(attr_vec)\n",
        "    for u in range(3):\n",
        "        sample_adj = gen_adj(G, base_adj.shape[0], int((base_adj).sum()) // 2, attr_vec)\n",
        "        # show_graph(sample_adj, base_adj)\n",
        "        diff_d = eval(sample_adj, base_adj=base_adj)\n",
        "        \n",
        "        for k in list(diff_d.keys()):\n",
        "            if k not in diffs.keys():\n",
        "                diffs[k] = []\n",
        "            diffs[k].append(diff_d[k])\n",
        "            \n",
        "for k in list(diffs.keys()):  \n",
        "    diffs[k] = np.mean(diffs[k])    \n",
        "print(diffs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph 12 5\n",
            "Graph 24 8\n",
            "Graph 13 6\n",
            "Graph 51 9\n",
            "Graph 33 8\n",
            "Graph 40 6\n",
            "Graph 45 8\n",
            "Graph 25 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Graph 15 5\n",
            "Graph 10 20\n",
            "Graph 22 4\n",
            "Graph 34 16\n",
            "Graph 39 4\n",
            "Graph 43 21\n",
            "Graph 23 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Graph 14 33\n",
            "Graph 27 36\n",
            "Graph 1 7\n",
            "Graph 3 12\n",
            "Graph 48 23\n",
            "Graph 16 33\n",
            "Graph 21 7\n",
            "Graph 9 6\n",
            "Graph 31 7\n",
            "Graph 49 21\n",
            "Graph 42 5\n",
            "{'d_max': 2.5384615384615383, 'd_min': 0.8974358974358975, 'd': 0.0, 'node_num': 0.0, 'edge_num': 0.0, 'LCC': 5.512820512820513, 'wedge_count': 48.333333333333336, 'claw_count': 193.44871794871796, 'triangle_count': 10.756410256410257, 'square_count': 7.269230769230769, 'power_law_exp': 0.7771615384615386, 'gini': 0.1964282051282051, 'rel_edge_distr_entropy': 0.1433448717948718, 'assortativity': nan, 'clustering_coefficient': 0.6223846153846154, 'n_components': 5.5, 'cpl': 1.179338461538462}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n",
            "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1BdMgJHDRqO",
        "colab_type": "code",
        "outputId": "93cb9a1f-0914-40b0-84dd-38a5cca73e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "pprint(diffs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LCC': 5.512820512820513,\n",
            " 'assortativity': nan,\n",
            " 'claw_count': 193.44871794871796,\n",
            " 'clustering_coefficient': 0.6223846153846154,\n",
            " 'cpl': 1.179338461538462,\n",
            " 'd': 0.0,\n",
            " 'd_max': 2.5384615384615383,\n",
            " 'd_min': 0.8974358974358975,\n",
            " 'edge_num': 0.0,\n",
            " 'gini': 0.1964282051282051,\n",
            " 'n_components': 5.5,\n",
            " 'node_num': 0.0,\n",
            " 'power_law_exp': 0.7771615384615386,\n",
            " 'rel_edge_distr_entropy': 0.1433448717948718,\n",
            " 'square_count': 7.269230769230769,\n",
            " 'triangle_count': 10.756410256410257,\n",
            " 'wedge_count': 48.333333333333336}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIl4xjiwDRlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g-liHF1a749",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li9V2Eb5vhxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9O69OwY5AcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(G)\n",
        "print(D)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}